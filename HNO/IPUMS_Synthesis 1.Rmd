---
title: "IPUMS Health Data"
author: "Henrik Olsson"
date: "February 25, 2020"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
subtitle: MATH 301 Data Confidentiality
---
```{r include=FALSE}
library(readr)
library(LearnBayes)
library(plyr)
library(dplyr)
library(ggplot2)
library(runjags)
library(coda)
#library(imputeTS)
library(fastDummies)
library(tinytex)
```

```{r}
## read data
data <- read.csv("nhis_00001.csv")
## REMOVE THIS LATER. ONLY TAKING 1000 SAMPLES B/C JAGS TOOK TOO LONG
data <- sample_n(data, 2000, replace = FALSE, prob = NULL)
## log income
## data$LOGINC<- log(data$EARNIMPOINT1)
#save(data,file="tp.RData")

load("tp.RData")

```

Our goal is to generate synthetic data from the estimated Bayesian synthesizer from the posterior predictive distribution. To produce a good synthesizer, there will be trade-offs between utility and risks. 


```{r include=FALSE}
## Remove missing observations 
na.remove(data$AGE)
na.remove(data$SEX)
na.remove(data$RACEA)
na.remove(data$EDUCREC2)
na.remove(data$HOURSWRK)
na.remove(data$EARNIMPOINT1)
na.remove(data$HINOTCOVE)
na.remove(data$HRSLEEP)
na.remove(data$WORFREQ)
```

```{r}
## Remove all NIU (00) values
data <- data[!data$EDUCREC2 == 00, ]
data <- data[!data$HOURSWRK == 00, ]
data <- data[!data$HINOTCOVE == 0, ]
data <- data[!data$HRSLEEP == 00, ]
data <- data[!data$WORFREQ == 0, ]
```

```{r include=FALSE}
## Remove all 96 (900), 97 (970), 98 (980), 99s (990)
data <- data[!data$RACEA == 900, ]
data <- data[!data$RACEA == 970, ]
data <- data[!data$RACEA == 980, ]
data <- data[!data$RACEA == 990, ]
data <- data[!data$EDUCREC2 == 96, ]
data <- data[!data$EDUCREC2 == 97, ]
data <- data[!data$EDUCREC2 == 98, ]
data <- data[!data$EDUCREC2 == 99, ]
data <- data[!data$HOURSWRK == 97, ]
data <- data[!data$HOURSWRK == 98, ]
data <- data[!data$HOURSWRK == 99, ]
data <- data[!data$HINOTCOVE == 7, ]
data <- data[!data$HINOTCOVE == 8, ]
data <- data[!data$HINOTCOVE == 9, ]
data <- data[!data$HRSLEEP == 97, ]
data <- data[!data$HRSLEEP == 98, ]
data <- data[!data$HRSLEEP == 99, ]
data <- data[!data$WORFREQ == 7, ]
data <- data[!data$WORFREQ == 8, ]
data <- data[!data$WORFREQ == 9, ]
```

```{r}
## Create new column RACE and recode into 6 categories
## 1 = White, 2 = Black, 3 = American Indian, 4 = Asian, 
## 5 = Other races, 6 = Two or more races
data <- data %>% mutate(RACE = ifelse(RACEA %in% 100, 1, ifelse(RACEA %in% 200, 2, ifelse(RACEA %in% c(300,310,320,330,340), 3, ifelse(RACEA %in% c(400,410,411,412,413,414,415,416,420,421,422,423,430,431,432,433,434), 4,  ifelse(RACEA %in% c(500,510,520,530,540,550,560,570,580,600,610,611,612,613,614,615,616,617), 5, 0))))))

## Create new column EDUC and recode into 3 categories
## 1 = 4 years of high school or less, 2 = 4 years of college,
## 3 = 5+ years of college
data <- data %>% mutate(EDUC = ifelse(EDUCREC2 %in% c(10,20,30,31,32,40,41,42), 1, ifelse(EDUCREC2 %in% c(50,51,52,53,54), 2, ifelse(EDUCREC2 %in% 60, 3, 0))))

data <- data %>% mutate(INCOME = ifelse(EARNIMPOINT1 %in% 0, 0, 1))
```

```{r}
head(data)
summary(data)
```


```{r include=FALSE}
## create indicator variable for sex
data$SEX = fastDummies::dummy_cols(data$SEX)
## create indicator variables for race
data$RACE = fastDummies::dummy_cols(data$RACE)
## create indicator variables for education
data$EDUC = fastDummies::dummy_cols(data$EDUC)
## create indicator variables for healthcare coverage
data$HEALTH = fastDummies::dummy_cols(data$HINOTCOVE)
## create indicator variables for how often feel worried, nervous, or anxious
data$WORRY = fastDummies::dummy_cols(data$WORFREQ)
```

## Step 1: Synthetic Logistic Regression Model

```{r}
## JAGS script
modelString <-"
model {
## sampling
for(i in 1:N){
y[i] ~ dbern(p[i])
logit(p[i]) <- beta0 + beta1*x_age[i] +
beta2*x_sex_male[i] + beta3*x_sex_female[i] +
beta4*x_race_w[i] + beta5*x_race_b[i] +
beta6*x_race_i[i] + beta7*x_race_a[i] +
beta8*x_race_o[i] +
beta10*x_educ_1[i] + beta11*x_educ_2[i] +
beta12*x_educ_3[i] + beta13*x_hourswrk[i] +
beta14*x_health_cov[i] + beta15*x_health_nocov[i] +
beta16*x_hrsleep[i] + beta17*x_wor_daily[i] +
beta18*x_wor_weekly[i] + beta19*x_wor_monthly[i] +
beta20*x_wor_fewtimes[i] + beta21*x_wor_never[i]
}
## priors
beta0 ~ dnorm(mu0, g0)
beta1 ~ dnorm(mu1, g1)
beta2 ~ dnorm(mu2, g2)
beta3 ~ dnorm(mu3, g3)
beta4 ~ dnorm(mu4, g4)
beta5 ~ dnorm(mu5, g5)
beta6 ~ dnorm(mu6, g6)
beta7 ~ dnorm(mu7, g7)
beta8 ~ dnorm(mu8, g8)
beta10 ~ dnorm(mu10, g10)
beta11 ~ dnorm(mu11, g11)
beta12 ~ dnorm(mu12, g12)
beta13 ~ dnorm(mu13, g13)
beta14 ~ dnorm(mu14, g14)
beta15 ~ dnorm(mu15, g15)
beta16 ~ dnorm(mu16, g16)
beta17 ~ dnorm(mu17, g17)
beta18 ~ dnorm(mu18, g18)
beta19 ~ dnorm(mu19, g19)
beta20 ~ dnorm(mu20, g20)
beta21 ~ dnorm(mu21, g21)
}"

#invsigma2 ~ dgamma(a, b)
#sigma <- sqrt(pow(invsigma2, -1))
```

```{r}
y = as.vector(data$INCOME)
x_age = as.vector(data$AGE) ## age
x_sex_male = as.vector(data$SEX$.data_1) ## male
x_sex_female = as.vector(data$SEX$.data_2) ## female
x_race_w = as.vector(data$RACE$.data_1) ## white
x_race_b = as.vector(data$RACE$.data_2) ## black/african-american
x_race_i = as.vector(data$RACE$.data_3) ## american indian
x_race_a = as.vector(data$RACE$.data_4) ## asian
x_race_o = as.vector(data$RACE$.data_5) ## other races
x_educ_1 = as.vector(data$EDUC$.data_3) ## 4 years of high school or less
x_educ_2 = as.vector(data$EDUC$.data_1) ## 4 years of college
x_educ_3 = as.vector(data$EDUC$.data_2) ## 5+ years of college
x_hourswrk = as.vector(data$HOURSWRK) ## hours of work
x_health_cov = as.vector(data$HEALTH$.data_1) ## has health coverage
x_health_nocov = as.vector(data$HEALTH$.data_2) ## has no health coverage
x_hrsleep = as.vector(data$HRSLEEP) ## hours of sleep
x_wor_daily = as.vector(data$WORRY$.data_2) ## worry daily
x_wor_weekly = as.vector(data$WORRY$.data_5) ## worry weekly
x_wor_monthly = as.vector(data$WORRY$.data_4) ## worry monthly
x_wor_fewtimes = as.vector(data$WORRY$.data_3) ## worry few times a year
x_wor_never = as.vector(data$WORRY$.data_1) ## worry never

N = length(y) # Compute the number of observations

## Pass the data and hyperparameter values to JAGS
the_data <- list("y" = y,
"x_age" = x_age, "x_sex_male" = x_sex_male,
"x_sex_female" = x_sex_female, "x_race_w" = x_race_w,
"x_race_b" = x_race_b, "x_race_i" = x_race_i,
"x_race_a" = x_race_a, "x_race_o" = x_race_o,
"x_educ_1" = x_educ_1,
"x_educ_2" = x_educ_2, "x_educ_3" = x_educ_3,
"x_hourswrk" = x_hourswrk, "x_health_cov" = x_health_cov,
"x_health_nocov" = x_health_nocov, "x_hrsleep" = x_hrsleep,
"x_wor_daily" = x_wor_daily, "x_wor_weekly" = x_wor_weekly,
"x_wor_monthly" = x_wor_monthly, "x_wor_fewtimes" = x_wor_fewtimes,
"x_wor_never" = x_wor_never, 
"N" = N,
"mu0" = 0, "g0" = 1, "mu1" = 0, "g1" = 1,
"mu2" = 0, "g2" = 1, "mu3" = 0, "g3" = 1,
"mu4" = 0, "g4" = 1, "mu5" = 0, "g5" = 1,
"mu6" = 0, "g6" = 1, "mu7" = 0, "g7" = 1,
"mu8" = 0, "g8" = 1, 
"mu10" = 0, "g10" = 1, "mu11" = 0, "g11" = 1,
"mu12" = 0, "g12" = 1, "mu13" = 0, "g13" = 1,
"mu14" = 0, "g14" = 1, "mu15" = 0, "g15" = 1,
"mu16" = 0, "g16" = 1, "mu17" = 0, "g17" = 1,
"mu18" = 0, "g18" = 1, "mu19" = 0, "g19" = 1,
"mu20" = 0, "g20" = 1, "mu21" = 0, "g21" = 1)
```

```{r}
initsfunction <- function(chain){
.RNG.seed <- c(1,2)[chain]
.RNG.name <- c("base::Super-Duper",
"base::Wichmann-Hill")[chain]
return(list(.RNG.seed=.RNG.seed,
.RNG.name=.RNG.name))
}

## Run the JAGS code for this model:
posterior_MLR <- run.jags(modelString,
n.chains = 1,
data = the_data,
monitor = c("beta0", "beta1", "beta2",
"beta3", "beta4", "beta5",
"beta6", "beta7", "beta8", "beta10",
"beta11", "beta12", "beta13", "beta14", "beta15", "beta16", "beta17",
"beta18", "beta19", "beta20", "beta21"),
adapt = 1000,
burnin = 5000,
sample = 5000,
thin = 1,
inits = initsfunction)
## JAGS output 
summary(posterior_MLR)
```

```{r}
plot(posterior_MLR, vars = "beta1")
```

```{r}
## Saving posterior parameter draws
post <- as.mcmc(posterior_MLR)
```

```{r}
post[1, "beta0"]
```

```{r}
## Generating one set of sythetic data
synthesize <- function(X, index, n){
  synthetic_Y <- c()
  for(i in 1:n){
     p <- plogis(post[index, "beta0"] +  X$x_age[i] * post[index, "beta1"] +  X$x_sex_male[i] * post[index, "beta2"] +  X$x_sex_female[i] * post[index, "beta3"] +  X$x_race_w[i] * post[index, "beta4"] +  X$x_race_b[i] * post[index, "beta5"] +  X$x_race_i[i] * post[index, "beta6"] +  X$x_race_a[i] * post[index, "beta7"] +  X$x_race_o[i] * post[index, "beta8"] +  X$x_educ_1[i] * post[index, "beta10"] +  X$x_educ_2[i] * post[index, "beta11"] +  X$x_educ_3[i] * post[index, "beta12"] +  X$x_hourswrk[i] * post[index, "beta13"] +  X$x_health_cov[i] * post[index, "beta14"] +  X$x_health_nocov[i] * post[index, "beta15"] +  X$x_hrsleep[i] * post[index, "beta16"] +  X$x_wor_daily[i] * post[index, "beta17"] +  X$x_wor_weekly[i] * post[index, "beta18"] +  X$x_wor_monthly[i] * post[index, "beta19"] +  X$x_wor_fewtimes[i] * post[index, "beta20"] +  X$x_wor_never[i] * post[index, "beta21"])
     synthetic_Y[i] <- rbinom(1,1,p)
  }
  data.frame(X$y, synthetic_Y)
}
n <- dim(data)[1]
X <- data.frame(y, x_age, x_sex_male, x_sex_female, x_race_w, x_race_b, x_race_i, x_race_a, x_race_o, x_educ_1, x_educ_2, x_educ_3, x_hourswrk, x_health_cov, x_health_nocov, x_hrsleep, x_wor_daily, x_wor_weekly, x_wor_monthly, x_wor_fewtimes, x_wor_never)

synthetic_one <- synthesize(params, 1, n)
names(synthetic_one) <- c("OrigLogIncome", "SynLogIncome")
```

```{r}
hist(synthetic_one$OrigLogIncome)
hist(synthetic_one$SynLogIncome)
```


```{r}
ggplot(synthetic_one, aes(x = OrigLogIncome, y = SynLogIncome)) + 
  geom_point(size = 1) + 
  labs(title = "Scatter plot of Synthetic log(Income) vs log(Income)") +
  theme_bw(base_size = 6, base_family = "")
```


## Utility Evaluation 

### Analysis-specific measures

#### Synthetic IPUMS sample: step 1


```{r message=FALSE, size = "footnotesize"}
n <- dim(data)[1]
m <- 20
synthetic_m <- vector("list",m)

for(i in 1:m){
  seed <- round(runif(1, 1, 1000))
  synthetic_new <- synthesize(params, 2000 + i, n)
  names(synthetic_new) <- c("params", "LogIncome")
  synthetic_m[[i]] <- synthetic_new
}
q <- rep(NA, m)
v <- rep(NA, m)
for (i in 1:m){
  synthetic_new <- synthetic_m[[i]]
  q[i] <- mean(synthetic_new$LogIncome)
  v[i] <- var(synthetic_new$LogIncome)/n
}
```

#### Synthetic IPUMS sample: step 2

```{r message=FALSE, size = "footnotesize"}
q_bar_m <- mean(q)
b_m <- var(q)
v_bar_m <- mean(v)
```

#### Synthetic IPUMS sample: step 3

```{r message=FALSE, size = "footnotesize"}
T_p <- b_m / m + v_bar_m
v_p <- (m - 1) * (1 + v_bar_m / (b_m / m))^2
```

#### Synthetic CE sample: step 4

- Obtain the point estimate for mean estimand $Q$, and the 95\% confidence interval

```{r message=FALSE, size = "footnotesize"}
set.seed(123)
q_bar_m
t_score_syn <- qt(p = 0.975, df = v_p)
c(q_bar_m - t_score_syn * sqrt(T_p), q_bar_m + t_score_syn * sqrt(T_p))
```
Synthetic mean estimand: 0.974
Synthetic 95% CI: [0.957, 0.990]

#### Synthetic CE sample: step 4-extra

```{r}
## Obtain the point estimate for mean estimand Q, and the 95% 
## confidence internval from the original data
set.seed(123)
mean_org <- mean(data$INCOME)
sd_org <- sd(data$INCOME)
t_score_org <- qt(p = 0.975, df = n-1)
mean_org
```

```{r}
set.seed(123)
c(mean_org - t_score_org * sd_org / sqrt(n), mean_org + t_score_org * sd_org / sqrt(n))
```

Original mean estimand: 0.970
Original 95% CI: [0.952, 0.987]

#### Interval overlap utility measure

Combining rules provide point estimate and confidence interval estimates. We can also obtain point estimate and confidence interval estimate from the original, confidential data.

```{r}
L_s = quantile(synthetic_new$params, 0.025)
U_s = quantile(synthetic_new$params, 0.975)
L_o = quantile(data$INCOME, 0.025)
U_o = quantile(data$INCOME, 0.975)
L_i = max(L_s, L_o)
U_i = min(U_s, U_o)
I = (U_i - L_i) / (2 * (U_o - L_o)) + (U_i - L_i)/ (2 * (U_s - L_s))
I
```

It seems like the interval overlap measure is 1 so we have high utility. 
We can potentially try to calculate multiple estimands and then take the average values of I over all estimands to obtain a summary.

## Identification Risk Evaluation 

First, we look into identification disclosure through expected match risk, true match rate, and false match rate. Then attribute disclosure will be examined. 

Higher expected match risk, higher true match rate, and lower false match rate indicate higher identification disclosure risk for the sample. Since we are doing a two-part synthesis model, we could potentially calculate three summaries on each synthetic dataset, and take the average. 

#### Step 1: calculate key quantities

```{r eval=FALSE, include=FALSE}
CalculateKeyQuantities <- function(data, params, known.vars, syn.vars, n){
  data <- data
  params <- params
  n <- n
  c_vector <- rep(NA, n)
  T_vector <- rep(NA, n)
  for (i in 1:n){
    match <- (eval(parse(text=paste("data$",syn.vars,"[i]==
                                      params$",syn.vars,sep="",collapse="&")))&
                  eval(parse(text=paste("data$",known.vars,"[i]==
                                        params$",known.vars,sep="",collapse="&"))))
    match.prob <- ifelse(match, 1/sum(match), 0)
    
    if (max(match.prob) > 0){
      c_vector[i] <- length(match.prob[match.prob == max(match.prob)])
    }
    else
      c_vector[i] <- 0
      T_vector[i] <- is.element(i, rownames(data)[match.prob == max(match.prob)])
  }
  
  K_vector <- (c_vector * T_vector == 1)
  F_vector <- (c_vector * (1 - T_vector) == 1)
  s <- length(c_vector[c_vector == 1 & is.na(c_vector) == FALSE])
  
  res_r <- list(c_vector = c_vector,
                T_vector = T_vector,
                K_vector = K_vector,
                F_vector = F_vector,
                s = s
  )
  return(res_r)
}
```

## Step 1: calculate key quantities cont'd 

- four synthesized variables: ```LANX, WAOB, DIS, HICOV```, assigned to ```syn.vars``` 
- three known variables: ```SEX, RACE, MAR```, assigned to ```known.vars```

```{r eval=FALSE, message=FALSE, include=FALSE, size=}
known.vars <- c("SEX", "RACE", "AGE", "EDUC","HOURSWRK", "HEALTH", "HRSLEEP","WORRY")
syn.vars <- c("INCOME")
n <- dim(data)[1]
KeyQuantities <- CalculateKeyQuantities(data, param, 
                                        known.vars, syn.vars, n)
```


## Step 2: calculate 3 summary measures

```{r eval=FALSE, message=FALSE, include=FALSE, size=}
IdentificationRisk <- function(c_vector, T_vector, K_vector, F_vector, s, N){
  
  nonzero_c_index <- which(c_vector > 0)
  exp_match_risk <- sum(1/c_vector[nonzero_c_index]*T_vector[nonzero_c_index])
  true_match_rate <- sum(na.omit(K_vector))/N
  false_match_rate <- sum(na.omit(F_vector))/s
  res_r <- list(exp_match_risk = exp_match_risk,
                true_match_rate = true_match_rate,
                false_match_rate = false_match_rate
  )
  return(res_r)
}
```

## Step 2: calculate 3 summary measures cont'd

- each record is a target, therefore ```N = n```

```{r eval=FALSE, message=FALSE, include=FALSE, size=}
c_vector <- KeyQuantities[["c_vector"]]
T_vector <- KeyQuantities[["T_vector"]]
K_vector <- KeyQuantities[["K_vector"]]
F_vector <- KeyQuantities[["F_vector"]]
s <- KeyQuantities[["s"]]
N <- n
ThreeSummaries <- IdentificationRisk(c_vector, T_vector, K_vector, F_vector, s, N)
```

## Step 2: calculate 3 summary measures cont'd

```{r eval=FALSE, message=FALSE, include=FALSE, size=}
ThreeSummaries[["exp_match_risk"]]
ThreeSummaries[["true_match_rate"]]
ThreeSummaries[["false_match_rate"]]
```

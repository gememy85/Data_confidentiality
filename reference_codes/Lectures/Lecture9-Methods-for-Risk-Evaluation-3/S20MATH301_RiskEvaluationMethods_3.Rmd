---
title: Methods for Risk Evaluation \#3
author: Jingchen (Monika) Hu 
institute: Vassar College
date: Data Confidentiality
output:
  beamer_presentation:
    includes:
      in_header: ../LectureStyle.tex
slide_level: 2
fontsize: 11pt

---


```{r setup, include=FALSE}
require(dplyr)
require(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

## Outline

\tableofcontents[hideallsubsections]

# Overview: attribute disclosure risks (AR)

## Overview

- Attribute disclosure refers to the intruder correctly inferring the true value(s) of synthesized variable(s) in the released synthetic datasets

- AR potentially exist in fully synthetic data and partially synthetic data

\pause

- Roadmap
    1. notations and setup
    2. key estimating steps (importance sampling)
    3. illustrative example: synthetic CE sample
    
# Notations and setup

## Notations and setup

- $\mathbf{y}_i = (y_{i1}, \cdots, y_{ip})$: the vector response of observation $i$ in the original confidential dataset, where direct identifiers (such as name or SSN) are removed

- When needed, we use $j$ as the variable index, and $j = 1, \cdots, p$. Among the $p$ variables
    - $\mathbf{y}_i^{s}$: synthesized variables
    - $\mathbf{y}_i^{us}$: un-synthesized variables

- $\mathbf{y}_i = (\mathbf{y}_i^{s}, \mathbf{y}_i^{us})$: the $i$-th observation

- $\mathbf{y} = (\mathbf{y}^{s}, \mathbf{y}^{us})$: the entire dataset containing $n$ observations
    - for fully synthetic data, $\mathbf{y}^{us} = \emptyset$, therefore $\mathbf{y} = \mathbf{y}^{s}$
    - without loss of generality, we use $\mathbf{y} = (\mathbf{y}^{s}, \mathbf{y}^{us})$
    
- $\mathbf{Z} = (\mathbf{Z}^{(1)}, \cdots, \mathbf{Z}^{(m)})$: $m > 1$ synthetic datasets
    
## Notations and setup cont'd

- Assumptions about intruder's knowledge and behavior
    1. the intruder intends to learn the value of $\mathbf{y}_i^{s}$ for some record $i$ in $\mathbf{y}$
    2. available information to the intruder:
        - $\mathbf{y}^{us} = \{\mathbf{y}_i^{us}: i = 1, \cdots, n\}$: the un-synthesized values of all $n$ observations
        - $A$: any auxiliary information known by the intruder about records in $\mathbf{y}$
        - $S$: any information known by the intruder about the process of generating $\mathbf{Z}$
        
## Notations and setup cont'd

- $\mathbf{Y}_i^{s}$: the random variable representing the intruder's uncertain knowledge of $\mathbf{y}_i^{s}$

- The intruder seeks the distribution: 
\begin{eqnarray}
p(\mathbf{Y}_i^{s} &\mid& \mathbf{Z}, \mathbf{y}^{us}, A, S) \\
p(\mathbf{Y}_i^{s} = \mathbf{y^*} &\mid& \mathbf{Z}, \mathbf{Y}^{us}, A, S)
\end{eqnarray}
    - if $\mathbf{Y}_i^{s}$ is a vector of categorical variables, consider $\mathbf{y^*}$ as one plausible combination of categorical responses of those variables in the neighborhood of $\mathbf{y}_i$
    - if $\mathbf{Y}_i^{s}$ is a vector of continuous variables, consider $\mathbf{y^*}$ as one plausible combination of continuous responses of those variables in the neighborhood of $\mathbf{y}_i$ *within certain distance*
    
## Notations and setup cont'd

- For the confidential data holder
    1. assumptions on the level of intruder's knowledge of $\mathbf{y}^{us}, A$, and $S$
    2. how to approximate $p(\mathbf{Y}_i^{s} = \mathbf{y^*} \mid \mathbf{Z}, \mathbf{Y}^{us}, A, S)$ (Bayesian thinking)
    
# Key estimating steps

## First step: Bayes' rule

\begin{eqnarray}
p(\mathbf{Y}_i^{s}  = \mathbf{y^*} \mid \mathbf{Z}, \mathbf{y}^{us}, A, S)
\propto p(\mathbf{Z} \mid \mathbf{Y}_i^{s} &=& \mathbf{y^*} , \mathbf{y}^{us}, A, S) \nonumber \\
p(\mathbf{Y}_i^{s} &=& \mathbf{y^*}  \mid \mathbf{y}^{us}, A, S)
\end{eqnarray}

- $\mathbf{y^*}$: one possible guess of $\mathbf{Y}_i^{s}$ by the intruder
- $\mathbf{y}^{us}$, $A$, and $S$: available to the intruder
- $p(\mathbf{Z} \mid \mathbf{Y}_i^{s} = \mathbf{y^*} , \mathbf{y}^{us}, A, S)$: the synthetic data distribution given what the intruder knows
- $p(\mathbf{Y}_i^{s} = \mathbf{y^*}  \mid \mathbf{y}^{us}, A, S)$: the intruder's prior on $\mathbf{Y}_i^{s} = \mathbf{y^*}$ given $\mathbf{y}^{us}$, $A$, and $S$

## Knowledge of $\mathbf{y}^{us}$

- $\mathbf{y}^{us} = \{\mathbf{y}_i^{us}: i = 1, \cdots, n\}$: the set of un-synthesized values of all $n$ observations

- Partial synthesis: intruder has access to $\mathbf{Z}$, therefore $\mathbf{y}^{us}$ can be determined and thus available

- Full synthesis: $\mathbf{y}^{us} = \emptyset$

\pause

- Without loss of generality, we keep $\mathbf{y}^{us}$

## Assumptions about $A$

- $A$: auxiliary information known by the intruder about records in $\mathbf{y}$

- Numerous possible scenarios

\pause 

- "Worst case": $A = \mathbf{y}_{-i}^{s}$
    - the intruder knows the original values of the synthesized variables of all records except for record $i$
    - strong intruder knowledge and conservative
    - if AR under such conservative assumption are acceptable, AR should be acceptable for weaker assumptions
    - realistic for computing purposes (more in detail later)
    
## Assumptions about $S$

- $S$: any information known by the intruder about the process of generating $\mathbf{Z}$

- Examples:
    1. code for the synthesizer
    2. descriptions of the synthesis model

\pause

- Such information sometimes can be public available with great details
    - recall the SynLBD synthesis paper
    
## Choosing the prior $p(\mathbf{Y}_i^{s}  = \mathbf{y^*} \mid \mathbf{y}^{us}, A, S)$

- Common practice: a uniform prior for all possible guesses $\mathbf{y^*}$

- Using a uniform prior cancels out the terms when comparing different guesses

\begin{eqnarray*}
p(\mathbf{Y}_i^{s}  = \mathbf{y^*} \mid \mathbf{Z}, \mathbf{y}^{us}, A, S)
\propto p(\mathbf{Z} \mid \mathbf{Y}_i^{s} &=& \mathbf{y^*} , \mathbf{y}^{us}, A, S) \nonumber \\
p(\mathbf{Y}_i^{s} &=& \mathbf{y^*}  \mid \mathbf{y}^{us}, A, S)
\end{eqnarray*}

- Do you think a uniform prior is reasonable? In what situation using it makes sense? When you might overestimate or underestimate the AR using uniform prior?

## The estimation of $p(\mathbf{Z} \mid \mathbf{Y}_i^{s} = \mathbf{y^*}, \mathbf{y}^{us}, A, S)$

- Independence between $\mathbf{Z}^{(l)}$'s:

\begin{eqnarray}
p(\mathbf{Z} \mid \mathbf{Y}_i^{s} = \mathbf{y^*}, \mathbf{y}^{us}, A, S) = \prod_{l=1}^{m} p(\mathbf{Z}^{(l)} \mid \mathbf{Y}_i^{s} = \mathbf{y^*}, \mathbf{y}^{us}, A, S)
\end{eqnarray}

- Work with each $\mathbf{Z}^{(l)}$

## The estimation of $p(\mathbf{Z} \mid \mathbf{Y}_i^{s} = \mathbf{y^*}, \mathbf{y}^{us}, A, S)$ cont'd

- Under the ``worst case" scenarior of $A = \mathbf{y}_{-i}^{s}$:

\begin{eqnarray}
p(\mathbf{Z}^{(l)} \mid \mathbf{Y}_i^{s}  = \mathbf{y^*}, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S),
\end{eqnarray}
which is very close to the distribution from which the synthetic data $\mathbf{Z}^{(l)}$ is generated, as in

\begin{eqnarray}
p(\mathbf{Z}^{(l)} \mid \mathbf{y}, S) = p(\mathbf{Z}^{(l)} \mid \mathbf{Y}_i^{s} = \mathbf{y}_i, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S)
\end{eqnarray}

- $\mathbf{y}_i$ is the true record in the original confidential dataset $\mathbf{y}$
- The difference between Equations (5) and (6)?
\pause
- The only difference in the conditioned quantities is difference between $\mathbf{y^*}$ (the random guess) and $\mathbf{y}_i$ (the true record)

## The estimation of $p(\mathbf{Z} \mid \mathbf{Y}_i^{s} = \mathbf{y^*}, \mathbf{y}^{us}, A, S)$ cont'd

- Monte Carlo approximation

- If we use $\Theta$ to denote the parameters in the synthesis model $M$, we could incorporate $\Theta$ draws in our estimation of $p(\mathbf{Z}^{(l)} \mid \mathbf{Y}_i^{s} = \mathbf{y}^*, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S)$

\begin{eqnarray*}
p(\mathbf{Z}^{(l)} \mid \mathbf{Y}_i^{s} = \mathbf{y}^*, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S) = \int p(\mathbf{Z}^{(l)} \mid \mathbf{Y}_i^{s} = \mathbf{y}^*, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S, \Theta) \nonumber \\
p(\Theta \mid \mathbf{Y}_i^{s} = \mathbf{y}^*, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S)d \Theta
\end{eqnarray*}

## The estimation of $p(\mathbf{Z} \mid \mathbf{Y}_i^{s} = \mathbf{y^*}, \mathbf{y}^{us}, A, S)$ cont'd

\begin{eqnarray*}
p(\mathbf{Z}^{(l)} \mid \mathbf{Y}_i^{s} = \mathbf{y}^*, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S) = \int p(\mathbf{Z}^{(l)} \mid \mathbf{Y}_i^{s} = \mathbf{y}^*, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S, \Theta) \nonumber \\
p(\Theta \mid \mathbf{Y}_i^{s} = \mathbf{y}^*, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S)d \Theta
\end{eqnarray*}

- The Monte Carlo step requires re-estimation of the synthesis model $M$ for each $\mathbf{Y}_i^{s} = \mathbf{y^*}$

- Could be computationally prohibitive if many possible guesses of $\mathbf{Y}_i^{s}$ need to be evaluated

\pause

- To avoid the re-estimation of $M$ to draw $\Theta$ samples, we can use the importance sampling strategy
    - available draws of $\Theta$ from $p(\Theta \mid \mathbf{y})$ (the model used for generating the synthetic dataset $\mathbf{Z}^{(l)}$) 
    - use them as proposals for the importance sampling algorithm
    
## The importance sampling strategy

- Suppose we seek to estimate the expectation of some function $g(\Theta)$, where $\Theta$ has density $f(\Theta)$

- Further suppose that we have a sample $(\Theta^{(1)}, \cdots, \Theta^{(H)})$ available from a convenient distribution $f^*(\Theta)$ that slightly differs from $f(\Theta)$

- We can estimate $E_f(g(\Theta))$ using

\begin{equation}
E_f(g(\Theta)) \approx \frac{1}{H} \sum_{h=1}^{H}g(\Theta^{(h)}) \frac{f(\Theta^{(h)}) / f^*(\Theta^{(h)})}{\sum_{h=1}^{H}f(\Theta^{(h)}) / f^*(\Theta^{(h)})}
\end{equation}

- We only require that $f(\Theta)$ and $f^*(\Theta)$ be known up to constants.

\pause

- What are our $f^*(\Theta)$ and $f(\Theta)$?

# Illustrative example: synthetic CE sample

## CE sample synthesis

```{r message=FALSE, size = "footnotesize"}
CEdata <- read.csv(file = "CEdata.csv")
CEdata$LogIncome <- log(CEdata$Income)
CEdata$LogExpenditure <- log(CEdata$Expenditure)
```

```{r message = FALSE, echo = FALSE, warning = FALSE, results = 'hide'}
require(runjags)
require(coda)

modelString <-"
model {
## sampling
for (i in 1:N){
y[i] ~ dnorm(beta0 + beta1*x[i], invsigma2)
}

## priors
beta0 ~ dnorm(mu0, g0)
beta1 ~ dnorm(mu1, g1)
invsigma2 ~ dgamma(a, b)
sigma <- sqrt(pow(invsigma2, -1))
}
"

y <- as.vector(CEdata$LogIncome)
x <- as.vector(CEdata$LogExpenditure)
N <- length(y)
the_data <- list("y" = y, "x" = x, "N" = N,
                 "mu0" = 0, "g0" = 0.0001,
                 "mu1" = 0, "g1" = 0.0001,
                 "a" = 1, "b" = 1)

initsfunction <- function(chain){
  .RNG.seed <- c(1,2)[chain]
  .RNG.name <- c("base::Super-Duper",
                 "base::Wichmann-Hill")[chain]
  return(list(.RNG.seed=.RNG.seed,
              .RNG.name=.RNG.name))
}

posterior <- run.jags(modelString,
                      n.chains = 1,
                      data = the_data,
                      monitor = c("beta0", "beta1", "sigma"),
                      adapt = 1000,
                      burnin = 5000,
                      sample = 5000,
                      thin = 50,
                      inits = initsfunction)

post <- as.mcmc(posterior)

synthesize_loginc <- function(X, index, n, seed){
  set.seed(seed)
  mean_Y <- post[index, "beta0"] +  X * post[index, "beta1"]
  synthetic_Y <- rnorm(n, mean_Y, post[index, "sigma"])
  data.frame(X, synthetic_Y)
}
```

```{r message=FALSE, size = "footnotesize"}
n <- dim(CEdata)[1]
synthetic_one <- synthesize_loginc(CEdata$LogExpenditure, 
                                   1, n, seed = 123)
names(synthetic_one) <- c("LogExpenditure", "LogIncome")
```

## AR calculation for CE sample

- ```m = 1``` for illustration

- Intruder knows each records' ```UrbanRural, Race, Expenditure``` (all un-synthesized variables)

- Intruder trys to use this information to infer the true values of the synthesized variable, ```Income```, based on the synthetic CE data in ```CEdata_syn```

```{r message=FALSE, size = "footnotesize"}
CEdata_org <- CEdata[, 1:4]
CEdata_syn <- as.data.frame(cbind(CEdata_org[, "UrbanRural"], 
                                  exp(synthetic_one
                                      [, "LogIncome"]),
                                  cbind(CEdata_org
                                        [, c("Race", 
                                             "Expenditure")])))
names(CEdata_syn) <- c("UrbanRural", "Income", 
                       "Race", "Expenditure")
```

## Estimating steps and assumptions

\begin{eqnarray}
p(Y_i^{s}  = y^* \mid \mathbf{Z}, \mathbf{y}^{us}, A, S)
\propto p(\mathbf{Z} \mid Y_i^{s} &=& y^* , \mathbf{y}^{us}, A, S) \nonumber \\
p(Y_i^{s} &=& y^*  \mid \mathbf{y}^{us}, A, S)
\end{eqnarray}

- $Y_i^{s}$: the univariate random variable represending the intruder's guess of the income of CU $i$
- $y^*$: one possible guess
- $\mathbf{Z}$: the synthetic CE sample (as in ```CEdata_syn```)
- $\mathbf{y}^{us}$: the set of un-synthesized values of all $n$ observations, which corresponds to the three un-synthesized variables ```UrbanRural, Race, Expenditure``` in the CE sample

## Estimating steps and assumptions cont'd

- $A = \mathbf{y}^{s}_{-i}$ (``worst case" scenario)

- $S$: the intruder knows that the synthesis model is a Bayesian linear regression

- $p(Y_i^{s} = y^*  \mid \mathbf{y}^{us}, A, S)$: assume a uniform prior, that is, all possible guesses of $y^*$ are equally likely

\begin{eqnarray}
p(\mathbf{Z} \mid Y_i^{s}  = y^*, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S)
\end{eqnarray}

## Estimating steps and assumptions cont'd

- Monte Carlo approximation

- $\Theta$: the parameters in the synthesis model $M$

\begin{eqnarray}
p(\mathbf{Z} \mid Y_i^{s} = y^*, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S) = \int p(\mathbf{Z} \mid Y_i^{s} = y^*, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S, \Theta) \nonumber \\
p(\Theta \mid Y_i^{s} = y^*, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S)d \Theta
\end{eqnarray}

- What are $\Theta$ in the CE example?

\pause

- $\Theta = \{\beta_0, \beta_1, \sigma\}$ in the Bayesian simple linear regression synthesis model $M$

## Estimating steps and assumptions cont'd

- The importance sampling strategy

\begin{equation*}
E_f(g(\Theta)) \approx \frac{1}{H} \sum_{h=1}^{H}g(\Theta^{(h)}) \frac{f(\Theta^{(h)}) / f^*(\Theta^{(h)})}{\sum_{h=1}^{H}f(\Theta^{(h)}) / f^*(\Theta^{(h)})}
\end{equation*}

- Define $g(\Theta)$:

\begin{eqnarray}
g(\Theta) = p(\mathbf{Z} \mid Y_i^{s} = y^*, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S)
\end{eqnarray}

- We approximate the expectation of each $g(\Theta)$ with respect to 

\begin{eqnarray}
f(\Theta) = p(\Theta \mid Y_i^{s} = y^*, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S)
\end{eqnarray}

- While trying to utilize samples $(\Theta^{(1)}, \cdots, \Theta^{(H)})$ from a convenient distribution
\begin{eqnarray}
f^*(\Theta) = p(\Theta \mid \mathbf{y}, S)
\end{eqnarray}

## Estimating steps and assumptions cont'd

- The importance sampling strategy

\begin{eqnarray}
g(\Theta^{(h)}) &=& p(\mathbf{Z} \mid Y_i^{s} = y^*, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S, \Theta^{(h)}) \nonumber \\
&=& \prod_{i=1}^{n} \left(\frac{1}{\sqrt{2 \pi}\sigma^{(h)}} \exp\left(-\frac{(\tilde{y}_i - \beta_0^{(h)} - \beta_1^{(h)} X_i)^2}{2(\sigma^{(h)})^2}\right)\right),
\end{eqnarray}

- $\tilde{y}_i$: the synthetic ```log(Income)```

- $X_i$: the un-synthesized ```log(Expenditure)``` of CU $i$ in the synthetic dataset $\mathbf{Z}$ (as in ```CEdata_syn```)

## Estimating steps and assumptions cont'd

- The importance sampling strategy

- Obtain $p(\mathbf{Z} \mid Y_i^{s} = y^*, \mathbf{y}^{us}, A = \mathbf{y}_{-i}^{s}, S) = \frac{1}{H} \sum_{h=1}^{H} p_h q_h$ where

\tiny

\begin{eqnarray*}
p_h &=& \prod_{i=1}^{n} \left(\frac{1}{\sqrt{2 \pi}\sigma^{(h)}} \exp\left(-\frac{(\tilde{y}_i - \beta_0^{(h)} - \beta_1^{(h)} X_i)^2}{2(\sigma^{(h)})^2}\right)\right) \\
q_h &=& \frac{\left(\frac{1}{\sqrt{2 \pi}\sigma^{(h)}} \exp\left(-\frac{(y^* - \beta_0^{(h)} - \beta_1^{(h)} X_i)^2}{2(\sigma^{(h)})^2}\right)\right) / \left(\frac{1}{\sqrt{2 \pi}\sigma^{(h)}} \exp\left(-\frac{(y_i - \beta_0^{(h)} - \beta_1^{(h)} X_i)^2}{2(\sigma^{(h)})^2}\right)\right)}{\sum_{h=1}^{H}\left( \left(\frac{1}{\sqrt{2 \pi}\sigma^{(h)}} \exp\left(-\frac{(y^* - \beta_0^{(h)} - \beta_1^{(h)} X_i)^2}{2(\sigma^{(h)})^2}\right)\right) / \left(\frac{1}{\sqrt{2 \pi}\sigma^{(h)}} \exp\left(-\frac{(y_i - \beta_0^{(h)} - \beta_1^{(h)} X_i)^2}{2(\sigma^{(h)})^2}\right)\right) \right)}
\end{eqnarray*}

\small

- $y^*$ is the guessed value
- $y_i$ is the true value for CU $i$'s ```log(Income)```

## Calculating $p(Y_i^{s}  = y^* \mid \mathbf{Z}, \mathbf{y}^{us}, A, S)$ for the CE example

- We need to work with the logarithm of ```Income``` and ```Expenditure``` in ```CEdata_org``` and ```CEdata_syn```
    - the model $M$ is fitted with logged continuous variables

- For ease of computation later, we round the logged values to 1 decimal point

```{r message=FALSE, size = "footnotesize"}
CEdata_org$LogIncome <- round(log(CEdata_org$Income), 
                              digits = 1)
CEdata_org$LogExpenditure <- round(log(CEdata_org$Expenditure), 
                                   digits = 1)
CEdata_syn$LogIncome <- round(log(CEdata_syn$Income), 
                              digits = 1)
CEdata_syn$LogExpenditure <- round(log(CEdata_syn$Expenditure), 
                                   digits = 1)
```

## Calculating $p(Y_i^{s}  = y^* \mid \mathbf{Z}, \mathbf{y}^{us}, A, S)$ for the CE example cont'd

- For illustration purpose, we demonstrate with CU 8: $y_i = 11.6, \tilde{y}_1 = 10.1, X_1 = 9.8$

```{r message=FALSE, size = "footnotesize"}
i <- 8
y_i <- CEdata_org$LogIncome[i]
y_i_guesses <- seq((y_i - 2.5), (y_i + 2.5), 0.5)
X_i <- CEdata_syn$LogExpenditure[i]
G <- length(y_i_guesses)
```

- Assume a collection of 11 possible guesses: $\{9.1, 9.6, 10.1, 10.6, 11.1, 11.6, 12.1, 12.6, 13.1, 13.6, 14.1\}$

- Use a uniform prior, $p(Y_i^{s} = y^*  \mid \mathbf{y}^{us}, A, S) = \frac{1}{11}$

## Calculating $p(Y_i^{s}  = y^* \mid \mathbf{Z}, \mathbf{y}^{us}, A, S)$ for the CE example cont'd

- Use the importance strategy with $H = 50$ parameter draws of $\Theta = \{\beta_0, \beta_1, \sigma\}$ from the Bayesian simple linear regression synthesis model

- The parameter draws are saved in ```post```

```{r message=FALSE, size = "footnotesize"}
H <- 50
beta0_draws <- post[1:H, "beta0"]
beta1_draws <- post[1:H, "beta1"]
sigma_draws <- post[1:H, "sigma"]
```

## Calculating $p(Y_i^{s}  = y^* \mid \mathbf{Z}, \mathbf{y}^{us}, A, S)$ for the CE example cont'd

- For computational stability, we use the ```compute_logsumexp()``` function below in calculating $\log(p_h q_h)$

- $p_h = \prod_{i=1}^{n} \left(\frac{1}{\sqrt{2 \pi}\sigma^{(h)}} \exp\left(-\frac{(\tilde{y}_i - \beta_0^{(h)} - \beta_1^{(h)} X_i)^2}{2(\sigma^{(h)})^2}\right)\right)$: take product of many normal pdfs

\begin{eqnarray}
\log\left(\sum_{i=1}^{n}\exp(x_i)\right) = a + \log\left(\sum_{i=1}^{n}\exp(x_i - a)\right),
\end{eqnarray}
where $a = \max_{i} x_i$.

```{r message=FALSE, size = "footnotesize"}
compute_logsumexp <- function(log_vector){
  log_vector_max <- max(log_vector)
  exp_vector <- exp(log_vector - log_vector_max)
  sum_exp <- sum(exp_vector)
  log_sum_exp <- log(sum_exp) + log_vector_max
  return(log_sum_exp)
}
```


## Calculating $p(Y_i^{s}  = y^* \mid \mathbf{Z}, \mathbf{y}^{us}, A, S)$ for the CE example cont'd

```{r message=FALSE, size = "footnotesize", eval = FALSE}
CU_i_logZ_all <- rep(NA, G)
for (g in 1:G){
  q_sum_H <- sum((dnorm(y_i_guesses[g], 
                        mean = (beta0_draws + beta1_draws * X_i), 
                        sd = sigma_draws)) / 
            (dnorm(y_i, mean = (beta0_draws + beta1_draws * X_i), 
                   sd = sigma_draws)))
  log_pq_h_all <- rep(NA, H)
  for (h in 1:H){
    log_p_h <- sum(log(dnorm(CEdata_syn$LogIncome, 
                             mean = (beta0_draws[h] + beta1_draws[h] * 
                                       CEdata_syn$LogExpenditure), 
                             sd = sigma_draws[h])))
```

## Calculating $p(Y_i^{s}  = y^* \mid \mathbf{Z}, \mathbf{y}^{us}, A, S)$ for the CE example cont'd

```{r message=FALSE, size = "footnotesize", eval = FALSE}
    log_q_h <- log(((dnorm(y_i_guesses[g], 
                           mean = (beta0_draws[h] + beta1_draws[h] * X_i), 
                           sd = sigma_draws[h])) / 
            (dnorm(y_i, mean = (beta0_draws[h] + beta1_draws[h] * X_i), 
                   sd = sigma_draws[h]))) / q_sum_H)
    log_pq_h_all[h] <- log_p_h + log_q_h
  }
  CU_i_logZ_all[g] <- compute_logsumexp(log_pq_h_all)
}
```


```{r message=FALSE, size = "footnotesize", echo = FALSE}
CU_i_logZ_all <- rep(NA, G)
for (g in 1:G){
  q_sum_H <- sum((dnorm(y_i_guesses[g], 
                        mean = (beta0_draws + beta1_draws * X_i), 
                        sd = sigma_draws)) / 
            (dnorm(y_i, mean = (beta0_draws + beta1_draws * X_i), 
                   sd = sigma_draws)))
  log_pq_h_all <- rep(NA, H)
  for (h in 1:H){
    log_p_h <- sum(log(dnorm(CEdata_syn$LogIncome, 
                             mean = (beta0_draws[h] + beta1_draws[h] * 
                                       CEdata_syn$LogExpenditure), 
                             sd = sigma_draws[h])))
  
    log_q_h <- log(((dnorm(y_i_guesses[g], 
                           mean = (beta0_draws[h] + beta1_draws[h] * X_i), 
                           sd = sigma_draws[h])) / 
            (dnorm(y_i, mean = (beta0_draws[h] + beta1_draws[h] * X_i), 
                   sd = sigma_draws[h]))) / q_sum_H)
    log_pq_h_all[h] <- log_p_h + log_q_h
  }
  CU_i_logZ_all[g] <- compute_logsumexp(log_pq_h_all)
}
```

## Calculating $p(Y_i^{s}  = y^* \mid \mathbf{Z}, \mathbf{y}^{us}, A, S)$ for the CE example cont'd

- With uniform prior, output ```CU_i_logZ_all``` is $\log(p(\mathbf{Z} \mid Y_i^{s} = y^*, \mathbf{y}^{us}, A, S)) \propto \log(p(Y_i^{s} = y^* \mid \mathbf{Z}, \mathbf{y}^{us}, A, S))$

- To re-normalize and obtain probabilities of each of $\log(p(Y_i^{s} = y^* \mid \mathbf{Z}, \mathbf{y}^{us}, A, S))$, we can apply the log-sum-exp trick again

```{r message=FALSE, size = "footnotesize", eval = FALSE}
prob <- exp(CU_i_logZ_all - max(CU_i_logZ_all)) / 
  sum(exp(CU_i_logZ_all - max(CU_i_logZ_all)))
outcome <- as.data.frame(cbind(y_i_guesses, prob))
names(outcome) <- c("guess", "probability")
outcome[order(outcome$probability, decreasing = TRUE), ]
```

## Calculating $p(Y_i^{s}  = y^* \mid \mathbf{Z}, \mathbf{y}^{us}, A, S)$ for the CE example cont'd

```{r message=FALSE, size = "footnotesize", echo = FALSE}
prob <- exp(CU_i_logZ_all - max(CU_i_logZ_all)) / 
  sum(exp(CU_i_logZ_all - max(CU_i_logZ_all)))
outcome <- as.data.frame(cbind(y_i_guesses, prob))
names(outcome) <- c("guess", "probability")
outcome[order(outcome$probability, decreasing = TRUE), ]
```

- The true value for CU 8, $y_i = 11.6$ (with $\tilde{y}_i = 10.1, X_i = 9.8$), has a probability of 0.0916 out of 1 to be guessed correctly, when compared to 10 other simular values in the neighborhood of 11.6

- It is ranked 4 among the 11 possible guesses

## Calculating $p(Y_i^{s}  = y^* \mid \mathbf{Z}, \mathbf{y}^{us}, A, S)$ for the CE example cont'd
As a comparison, CU 10 ($y_i = 11.6, \tilde{y}_i = 10.7, X_i = 9.5$)

```{r message=FALSE, size = "footnotesize", echo = FALSE}
i <- 10
y_i <- CEdata_org$LogIncome[i]
y_i_guesses <- seq((y_i - 2.5), (y_i + 2.5), 0.5)
X_i <- CEdata_syn$LogExpenditure[i]
G <- length(y_i_guesses)

CU_i_logZ_all <- rep(NA, G)
for (g in 1:G){
  q_sum_H <- sum((dnorm(y_i_guesses[g], 
                        mean = (beta0_draws + beta1_draws * X_i), 
                        sd = sigma_draws)) / 
            (dnorm(y_i, mean = (beta0_draws + beta1_draws * X_i), 
                   sd = sigma_draws)))
  
  log_pq_h_all <- rep(NA, H)
  for (h in 1:H){
    log_p_h <- sum(log(dnorm(CEdata_syn$LogIncome, 
                             mean = (beta0_draws[h] + beta1_draws[h] * 
                                       CEdata_syn$LogExpenditure), 
                             sd = sigma_draws[h])))
  
    log_q_h <- log(((dnorm(y_i_guesses[g], 
                           mean = (beta0_draws[h] + beta1_draws[h] * X_i), 
                           sd = sigma_draws[h])) / 
            (dnorm(y_i, mean = (beta0_draws[h] + beta1_draws[h] * X_i), 
                   sd = sigma_draws[h]))) / q_sum_H)
    
    log_pq_h_all[h] <- log_p_h + log_q_h
  }
  
  CU_i_logZ_all[g] <- compute_logsumexp(log_pq_h_all)
}

prob <- exp(CU_i_logZ_all - max(CU_i_logZ_all)) / 
  sum(exp(CU_i_logZ_all - max(CU_i_logZ_all)))
outcome <- as.data.frame(cbind(y_i_guesses, prob))
names(outcome) <- c("guess", "probability")
outcome[order(outcome$probability, decreasing = TRUE), ]
```

## Final comments

- We can repeat this calculation process for all $i \in 1, \cdots, n = 994$ observations in the CE sample (write a function)

- Report the normalized probability of the true value being guessed correctly, as well as its ranking among the 11 possible guesses within the neighborhood

- Summarize / visualize the distributions of probability and rank